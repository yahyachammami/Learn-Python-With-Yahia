{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef72235",
   "metadata": {},
   "source": [
    "#  Natural Language Processing Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720b0a5a",
   "metadata": {},
   "source": [
    "## Yahia Chammami"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54598d5",
   "metadata": {},
   "source": [
    "## I-Natural Language Toolkit  (NLTK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf1c92e",
   "metadata": {},
   "source": [
    "### 1. Introduction to nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342cac3e",
   "metadata": {},
   "source": [
    "**NLTK is a versatile library that is commonly used for various NLP tasks, including text classification, sentiment analysis, information extraction, and text generation. It serves as a valuable resource for researchers, developers, and data scientists working with natural language data in Python.**\n",
    "\n",
    "**Corpora** : NLTK includes a vast collection of linguistic corpora, such as the Penn Treebank, WordNet, and various text collections. These corpora serve as valuable resources for linguistic research and NLP tasks.\n",
    "\n",
    "**Text Processing Libraries**: NLTK provides a wide range of text processing tools and modules, including tokenization, stemming, lemmatization, part-of-speech tagging, parsing, and more. These tools enable you to preprocess and analyze text data effectively.\n",
    "\n",
    "**Machine Learning**: NLTK includes utilities and algorithms for text classification, sentiment analysis, and other machine learning-based NLP tasks.\n",
    "\n",
    "**Linguistic Resources**: The library offers access to lexical resources like WordNet, which is a large lexical database of English, and various language grammars and parsers.\n",
    "\n",
    "**Text Corpora and Lexical Resources**: NLTK provides access to a variety of text corpora and lexical resources, including dictionaries and thesauri.\n",
    "\n",
    "**Natural Language Processing and Linguistics Algorithms**: It includes various algorithms for tasks such as parsing, semantic reasoning, and machine translation.\n",
    "\n",
    "**Visualization and Tools**: NLTK offers tools for visualization and exploration of linguistic data.\n",
    "\n",
    "**Community and Resources**: NLTK has a large and active user community, along with extensive documentation and educational resources. It’s widely used in academia and industry for NLP research and applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24652e2c",
   "metadata": {},
   "source": [
    "### 2. Corpora(Corpus)\n",
    "**In NLTK (Natural Language Toolkit), a corpus (plural: corpora) refers to a large and structured\n",
    "collection of text or speech data. Corpora in NLTK are used for various natural language processing\n",
    "(NLP) tasks, including linguistic research, text analysis, and the development of NLP models and\n",
    "algorithms. These corpora are often used as training and testing data for NLP tasks, and they\n",
    "provide researchers and practitioners with a broad range of textual resources for different languages\n",
    "and domains.**\n",
    "\n",
    "NLTK includes various built-in **corpora** that cover different domains, languages, and types of text\n",
    "data. Some of the most commonly used corpora in NLTK include:\n",
    "\n",
    "**Gutenberg Corpus**: A collection of classic literary texts, such as novels and essays, fromthe Project Gutenberg digital library.\n",
    "**Brown Corpus**: A corpus of American English text from diverse sources, classified into\n",
    "numerous genres and used for linguistic research.\n",
    "**Inaugural Address Corpus**: A collection of U.S. presidential inaugural addresses, useful for studying the language used in political speeches.\n",
    "**WordNet**: While not a text corpus, WordNet is a lexical database that NLTK provides access to. It’s a resource for looking up word meanings, synonyms, antonyms, and other lexical information.\n",
    "**Penn Treebank Corpus**: A collection of newspaper text with part-of-speech tagging, syntactic parsing, and other linguistic annotations.\n",
    "**Reuters Corpus**: A collection of news articles from the Reuters news agency, often used for text classification and information retrieval tasks.\n",
    "**Movie Reviews Corpus**: A collection of movie reviews categorized as positive or negative, frequently used for sentiment analysis and text classification.\n",
    "**Chat-80 Data**: A corpus of chat-room-style conversations.\n",
    "**Web Text Corpus**: A corpus containing text from a variety of web sources.\n",
    "\n",
    "**These corpora serve different purposes in linguistic research and NLP tasks, such as text classification, language modeling, sentiment analysis, and more. NLTK provides tools and methods\n",
    "to access and work with these corpora, making it a valuable resource for NLP practitioners and\n",
    "researchers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0ca8e471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# This will open the NLTK downloader GUI\n",
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225a7d50",
   "metadata": {},
   "source": [
    "**The nltk.download() command opens the NLTK Data downloader, allowing you to download various datasets and resources that NLTK uses.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "da4925aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "# Get the categories (labels)\n",
    "movie_reviews.categories()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621e7af0",
   "metadata": {},
   "source": [
    "**The movie_reviews corpus in NLTK contains movie reviews categorized as positive and negative. The categories() function is used to obtain the categories or labels associated with the reviews. In the case of the movie_reviews corpus, there are two categories: 'pos' for positive reviews and 'neg' for negative reviews.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c0c9b10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party']\n"
     ]
    }
   ],
   "source": [
    "# Get the words from the movie_reviews corpus\n",
    "all_words = movie_reviews.words()\n",
    "\n",
    "# Print the first 10 words\n",
    "print(all_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169b2a23",
   "metadata": {},
   "source": [
    "**The movie_reviews.words() function provides a flat list of words, and you can use it to perform various text processing tasks, such as frequency analysis, feature extraction, and more.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce8fef6",
   "metadata": {},
   "source": [
    "## II- Text Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf2cfe2",
   "metadata": {},
   "source": [
    "Data pre-processing is the process of making the machine understand things better or making the\n",
    "input more machine understandable. Some standard practices for doing that are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae8b3a5",
   "metadata": {},
   "source": [
    "### 1.Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b012744",
   "metadata": {},
   "source": [
    "**Tokenization is the process of breaking a text into individual words or “tokens.” NLTK (Natural\n",
    "Language Toolkit) provides various methods for tokenizing text in Python. Here’s how you can\n",
    "tokenize text using NLTK**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bf19cd",
   "metadata": {},
   "source": [
    "#### a) Using NLTK’s Default Tokenizer:\n",
    "NLTK comes with a default tokenizer called word_tokenize, which can be used to split text\n",
    "into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "214d45b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yahia', 'knows', 'that', 'Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'text', 'into', 'words', 'or', 'phrases', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Yahia knows that Tokenization is the process of breaking down text into words or phrases.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56da5f4d",
   "metadata": {},
   "source": [
    "#### b) Using NLTK’s Sentence Tokenizer:\n",
    "If you want to split text into sentences, you can use NLTK’s sent_tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8972b586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is the first sentence.', 'And this is the second one!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"This is the first sentence. And this is the second one!\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcd6294",
   "metadata": {},
   "source": [
    "#### c) Custom Tokenization:\n",
    "You can also create a custom tokenizer using regular expressions to split text based on specific\n",
    "patterns. For example, you can tokenize text based on spaces and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "975f1bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Custom', 'tokenization', 'can', 'be', 'done', 'with', 'regular', 'expressions', 'For', 'example', 'split', 'text', 'based', 'on', 'spaces', 'and', 'punctuation']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Custom tokenization can be done with regular expressions. For example, split text based on spaces and punctuation!\"\n",
    "# Tokenize based on spaces and punctuation\n",
    "tokens = re.split(r'\\s+|[,;.!]', text)\n",
    "# Remove empty strings\n",
    "tokens = [token for token in tokens if token]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d56f3",
   "metadata": {},
   "source": [
    "The choice of tokenizer depends on your specific NLP task and the characteristics of the text you\n",
    "are working with. NLTK provides flexibility and allows you to use the tokenizer that best fits\n",
    "your needs, whether it’s the default tokenizer, a sentence tokenizer, or a custom tokenizer based on\n",
    "regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c12d685d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'pledge', 'to', 'be', 'a', 'data', 'scientist', 'one', 'day']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "data = \"I pledge to be a data scientist one day\"\n",
    "tokenized_text=word_tokenize(data)\n",
    "print(tokenized_text)\n",
    "print(type(tokenized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0ff78ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cake is a form of sweet food made from flour sugar ,and other ingredients,\\nthat is usually baked.In their oldest forms, cakes were modifications of bread, \\nbut cakes now cover a wide range of preparationsthat can be simple or elaborate,\\nand that share features with other dessertssuch as pastries, meringues, custards,\\nand pies.The most commonly used cakeingredients include flour,\\nsugar, eggs, butter or oil or margarine, a liquid, and leavening agents,\\nsuch as baking soda or baking powder.', 'Common additional ingredients and flavourings include dried, candied, or fresh\\nfruit, nuts, cocoa, and extracts such as vanilla, with numerous\\nsubstitutions for the primary ingredients.Cakes can also be filled with\\nfruit preserves, nuts or dessert sauces (like pastry cream), iced with\\nbuttercream or other icings, and decorated with marzipan, piped borders, or\\ncandied fruit.']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "para=\"\"\"Cake is a form of sweet food made from flour sugar ,and other ingredients,\n",
    "that is usually baked.In their oldest forms, cakes were modifications of bread, \n",
    "but cakes now cover a wide range of preparationsthat can be simple or elaborate,\n",
    "and that share features with other dessertssuch as pastries, meringues, custards,\n",
    "and pies.The most commonly used cakeingredients include flour,\n",
    "sugar, eggs, butter or oil or margarine, a liquid, and leavening agents,\n",
    "such as baking soda or baking powder. Common additional ingredients and flavourings include dried, candied, or fresh\n",
    "fruit, nuts, cocoa, and extracts such as vanilla, with numerous\n",
    "substitutions for the primary ingredients.Cakes can also be filled with\n",
    "fruit preserves, nuts or dessert sauces (like pastry cream), iced with\n",
    "buttercream or other icings, and decorated with marzipan, piped borders, or\n",
    "candied fruit.\"\"\"\n",
    "tokenized_para=sent_tokenize(para)\n",
    "print(tokenized_para)\n",
    "print(type(tokenized_para))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7034dc7b",
   "metadata": {},
   "source": [
    "### 2. Punctuation Removal\n",
    "Removing punctuation from text is a common preprocessing step in natural language processing\n",
    "(NLP) tasks. Punctuation removal helps simplify text and can be useful for various NLP tasks\n",
    "like text classification, text analysis, and text mining. You can remove punctuation from text in\n",
    "Python using various methods, including regular expressions and string manipulation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe6c0aa",
   "metadata": {},
   "source": [
    "**Here’s how to remove punctuation from text using Python:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56dbb5b",
   "metadata": {},
   "source": [
    "#### a) Using Regular Expressions:\n",
    "You can use the re library to remove punctuation using regular expressions. In this example,\n",
    "we’ll remove all non-alphanumeric characters (i.e., remove everything that is not a letter or\n",
    "a number):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c2dc2dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World This is an example text with some punctuation\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, World! This is an example text with some punctuation.\"\n",
    "# Remove non-alphanumeric characters using regular expression\n",
    "text_without_punctuation = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n",
    "print(text_without_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea23ae6",
   "metadata": {},
   "source": [
    "#### b) Using String Manipulation:\n",
    "You can also remove punctuation by iterating through each character in the text and keeping\n",
    "only the characters that are letters or spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7932379d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World This is an example text with some punctuation\n"
     ]
    }
   ],
   "source": [
    "# Input text containing punctuation\n",
    "text = \"Hello, World! This is an example text with some punctuation.\"\n",
    "\n",
    "# Remove punctuation using string manipulation\n",
    "text_without_punctuation = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "\n",
    "# Print the result (text without punctuation)\n",
    "print(text_without_punctuation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd3f90",
   "metadata": {},
   "source": [
    "#### c) Using the string Module:\n",
    "Python’s string module provides a string of all punctuation characters. You can use this\n",
    "module to remove punctuation from text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9c6fdafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World This is an example text with some punctuation\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Input text containing punctuation\n",
    "text = \"Hello, World! This is an example text with some punctuation.\"\n",
    "\n",
    "# Create a translator using str.maketrans to remove punctuation\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# Remove punctuation from the text using the translator\n",
    "text_without_punctuation = text.translate(translator)\n",
    "\n",
    "# Print the result (text without punctuation)\n",
    "print(text_without_punctuation)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a9cc5a2",
   "metadata": {},
   "source": [
    "Choose the method that best fits your specific NLP task and text data. Punctuation removal is\n",
    "often an essential step in text preprocessing to ensure that the text is in a clean and suitable format\n",
    "for analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "59667138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wow', 'I', 'am', 'excited', 'to', 'learn', 'data', 'science']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Initialize a RegexpTokenizer with a regular expression pattern\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Input text\n",
    "text = \"Wow! I am excited to learn data science\"\n",
    "\n",
    "# Tokenize the text using the defined regular expression pattern\n",
    "result = tokenizer.tokenize(text)\n",
    "\n",
    "# Print the result (list of words)\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61639d34",
   "metadata": {},
   "source": [
    "### 3. Stop Words Removal\n",
    "Stop words are words which occur frequently in a corpus. e.g a, an, the, in. Frequently occurring\n",
    "words are removed from the corpus for the sake of text-normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cae8c58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cake', 'is', 'a', 'form', 'of', 'sweet', 'food', 'made', 'from', 'flour', ',', 'sugar', ',', 'and', 'other', 'ingredients', ',', 'that', 'is', 'usually', 'baked', '.', 'In', 'their', 'oldest', 'forms', ',', 'cakes', 'were', 'modifications', 'of', 'bread', ',', 'but', 'cakes', 'now', 'cover', 'a', 'wide', 'range', 'of', 'preparations', 'that', 'can', 'be', 'simple', 'or', 'elaborate', ',', 'and', 'that', 'share', 'features', 'with', 'other', 'desserts', 'such', 'as', 'pastries', ',', 'meringues', ',', 'custards', ',', 'and', 'pies', '.']\n",
      "['Cake', 'form', 'sweet', 'food', 'made', 'flour', ',', 'sugar', ',', 'ingredients', ',', 'usually', 'baked', '.', 'In', 'oldest', 'forms', ',', 'cakes', 'modifications', 'bread', ',', 'cakes', 'cover', 'wide', 'range', 'preparations', 'simple', 'elaborate', ',', 'share', 'features', 'desserts', 'pastries', ',', 'meringues', ',', 'custards', ',', 'pies', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Get the set of English stopwords\n",
    "to_be_removed = set(stopwords.words('english'))\n",
    "\n",
    "# Input paragraph containing multiple sentences\n",
    "para = \"\"\"Cake is a form of sweet food made from flour, sugar, and other ingredients, that is usually baked.\n",
    "In their oldest forms, cakes were modifications of bread, but cakes now cover a wide range of preparations\n",
    "that can be simple or elaborate, and that share features with other desserts such as pastries, meringues, custards,\n",
    "and pies.\"\"\"\n",
    "\n",
    "# Tokenize the paragraph into words\n",
    "tokenized_para = word_tokenize(para)\n",
    "print(tokenized_para)\n",
    "\n",
    "# Remove stopwords from the tokenized list\n",
    "modified_token_list = [word for word in tokenized_para if not word in to_be_removed]\n",
    "\n",
    "# Print the modified token list (without stopwords)\n",
    "print(modified_token_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815e5b53",
   "metadata": {},
   "source": [
    "### 4. Stemming\n",
    "It is reduction of inflection from words. Words with same origin will get reduced to a form which\n",
    "may or may not be a word.\n",
    "#### a) Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c39fb784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cake', 'is', 'a', 'form', 'of', 'sweet', 'food', 'made', 'from', 'flour', ',', 'sugar', ',', 'and', 'other', 'ingredi', ',', 'that', 'is', 'usual', 'bake', '.', 'in', 'their', 'oldest', 'form', ',', 'cake', 'were', 'modif', 'of', 'bread', ',', 'but', 'cake', 'now', 'cover', 'a', 'wide', 'rang', 'of', 'prepar', 'that', 'can', 'be', 'simpl', 'or', 'elabor', ',', 'and', 'that', 'share', 'featur', 'with', 'other', 'dessert', 'such', 'as', 'pastri', ',', 'meringu', ',', 'custard', ',', 'and', 'pie', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Input text containing multiple sentences\n",
    "content = \"\"\"Cake is a form of sweet food made from flour, sugar, and other ingredients,\n",
    "that is usually baked. In their oldest forms, cakes were modifications of bread,\n",
    "but cakes now cover a wide range of preparations that can be simple or elaborate,\n",
    "and that share features with other desserts such as pastries, meringues, custards, and pies.\"\"\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokenized_content = word_tokenize(content)\n",
    "\n",
    "# Apply stemming to each word\n",
    "stemmed_words = [stemmer.stem(word) for word in tokenized_content]\n",
    "\n",
    "# Print the stemmed words\n",
    "print(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9937dc9",
   "metadata": {},
   "source": [
    "#### b) Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2bf07957",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cak', 'is', 'a', 'form', 'of', 'sweet', 'food', 'mad', 'from', 'flo', ',', 'sug', ',', 'and', 'oth', 'ingredy', ',', 'that', 'is', 'us', 'bak', '.', 'in', 'their', 'oldest', 'form', ',', 'cak', 'wer', 'mod', 'of', 'bread', ',', 'but', 'cak', 'now', 'cov', 'a', 'wid', 'rang', 'of', 'prep', 'that', 'can', 'be', 'simpl', 'or', 'elab', ',', 'and', 'that', 'shar', 'feat', 'with', 'oth', 'dessert', 'such', 'as', 'pastry', ',', 'meringu', ',', 'custard', ',', 'and', 'pie', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Initialize the Lancaster Stemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# Input text containing multiple sentences\n",
    "content = \"\"\"Cake is a form of sweet food made from flour, sugar, and other ingredients, that is usually baked.\n",
    "In their oldest forms, cakes were modifications of bread, but cakes now cover a wide range of preparations\n",
    "that can be simple or elaborate, and that share features with other desserts such as pastries, meringues, custards,\n",
    "and pies.\"\"\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokenized_content = word_tokenize(content)\n",
    "\n",
    "# Apply stemming to each word\n",
    "stemmed_words = [stemmer.stem(word) for word in tokenized_content]\n",
    "\n",
    "# Print the stemmed words\n",
    "print(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb5ded7",
   "metadata": {},
   "source": [
    "### 5. Lemmatization\n",
    "It is another process of reducing inflection from words. The way its different from stemming is that\n",
    "it reduces words to their origins which have actual meaning. Stemming sometimes generates words\n",
    "which are not even words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "610c8773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cake', 'is', 'a', 'form', 'of', 'sweet', 'food', 'made', 'from', 'flour', ',', 'sugar', ',', 'and', 'other', 'ingredient', ',', 'that', 'is', 'usually', 'baked', '.', 'In', 'their', 'oldest', 'form', ',', 'cake', 'were', 'modification', 'of', 'bread', ',', 'but', 'cake', 'now', 'cover', 'a', 'wide', 'range', 'of', 'preparation', 'that', 'can', 'be', 'simple', 'or', 'elaborate', ',', 'and', 'that', 'share', 'feature', 'with', 'other', 'dessert', 'such', 'a', 'pastry', ',', 'meringue', ',', 'custard', ',', 'and', 'pie', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Input text containing multiple sentences\n",
    "content = \"\"\"Cake is a form of sweet food made from flour, sugar, and other ingredients, that is usually baked.\n",
    "In their oldest forms, cakes were modifications of bread, but cakes now cover a wide range of preparations\n",
    "that can be simple or elaborate, and that share features with other desserts such as pastries, meringues, custards,\n",
    "and pies.\"\"\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokenized_content = word_tokenize(content)\n",
    "\n",
    "# Lemmatize each word\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokenized_content]\n",
    "\n",
    "# Print the lemmatized words\n",
    "print(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ad6acc",
   "metadata": {},
   "source": [
    "### 6. POS Tagging\n",
    "POS tagging is the process of identifying parts of speech of a sentence. It is able to identify nouns,\n",
    "pronouns, adjectives etc. in a sentence and assigns a POS token to each word. There are different\n",
    "methods to tag, but we will be using the universal style of tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "475c1800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Cake', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('form', 'NOUN'), ('of', 'ADP'), ('sweet', 'ADJ'), ('food', 'NOUN'), ('made', 'VERB'), ('from', 'ADP'), ('flour', 'NOUN'), (',', '.'), ('sugar', 'NOUN'), (',', '.'), ('and', 'CONJ'), ('other', 'ADJ'), ('ingredients', 'NOUN'), (',', '.'), ('that', 'DET'), ('is', 'VERB'), ('usually', 'ADV'), ('baked', 'VERB'), ('.', '.')], [('In', 'ADP'), ('their', 'PRON'), ('oldest', 'ADJ'), ('forms', 'NOUN'), (',', '.'), ('cakes', 'NOUN'), ('were', 'VERB'), ('modifications', 'NOUN'), ('of', 'ADP'), ('bread', 'NOUN'), (',', '.'), ('but', 'CONJ'), ('cakes', 'NOUN'), ('now', 'ADV'), ('cover', 'VERB'), ('a', 'DET'), ('wide', 'ADJ'), ('range', 'NOUN'), ('of', 'ADP'), ('preparations', 'NOUN'), ('that', 'DET'), ('can', 'VERB'), ('be', 'VERB'), ('simple', 'ADJ'), ('or', 'CONJ'), ('elaborate', 'ADJ'), (',', '.'), ('and', 'CONJ'), ('that', 'ADP'), ('share', 'NOUN'), ('features', 'NOUN'), ('with', 'ADP'), ('other', 'ADJ'), ('desserts', 'NOUN'), ('such', 'ADJ'), ('as', 'ADP'), ('pastries', 'NOUN'), (',', '.'), ('meringues', 'NOUN'), (',', '.'), ('custards', 'NOUN'), (',', '.'), ('and', 'CONJ'), ('pies', 'NOUN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Input text containing multiple sentences\n",
    "content = \"\"\"Cake is a form of sweet food made from flour, sugar, and other ingredients, that is usually baked.\n",
    "In their oldest forms, cakes were modifications of bread, but cakes now cover a wide range of preparations\n",
    "that can be simple or elaborate, and that share features with other desserts such as pastries, meringues, custards,\n",
    "and pies.\"\"\"\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = sent_tokenize(content)\n",
    "\n",
    "# Tokenize each sentence into words\n",
    "words = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# Perform part-of-speech tagging using the 'universal' tagset\n",
    "pos_tags = [nltk.pos_tag(sentence, tagset=\"universal\") for sentence in words]\n",
    "\n",
    "# Print the result (list of sentences, each containing words with their POS tags)\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b06bb2f",
   "metadata": {},
   "source": [
    "### 7. Chunking\n",
    "Chunking also known as shallow parsing, is practically a method in NLP applied to POS tagged data\n",
    "to gain further insights from it. It is done by grouping certain words on the basis of a pre-defined\n",
    "rule. The text is then parsed according to the rule to group data for phrase creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "89c09de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Cake/NNP\n",
      "  is/VBZ\n",
      "  (NP a/DT form/NN)\n",
      "  of/IN\n",
      "  (NP sweet/JJ food/NN)\n",
      "  made/VBN\n",
      "  from/IN\n",
      "  (NP flour/NN)\n",
      "  ,/,\n",
      "  (NP sugar/NN)\n",
      "  ,/,\n",
      "  and/CC\n",
      "  other␣/JJ\n",
      "  ↪ingredients/NNS\n",
      "  ,/,\n",
      "  that/DT\n",
      "  is/VBZ\n",
      "  usually/RB\n",
      "  baked/VBN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Input text\n",
    "content = \"Cake is a form of sweet food made from flour, sugar, and other␣\\n↪ingredients, that is usually baked.\"\n",
    "\n",
    "# Tokenize the input text into words\n",
    "tokenized_text = nltk.word_tokenize(content)\n",
    "\n",
    "# Perform part-of-speech tagging on the tokenized words\n",
    "tagged_token = nltk.pos_tag(tokenized_text)\n",
    "\n",
    "# Define a simple grammar for NP (Noun Phrase) extraction\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# Create a regular expression parser based on the defined grammar\n",
    "phrases = nltk.RegexpParser(grammar)\n",
    "\n",
    "# Apply the parser to the tagged tokens to extract noun phrases\n",
    "result = phrases.parse(tagged_token)\n",
    "\n",
    "# Print the result (the parsed tree structure)\n",
    "print(result)\n",
    "\n",
    "# Visualize the result by drawing the parsed tree\n",
    "result.draw()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606a4fa5",
   "metadata": {},
   "source": [
    "### 8. Word Embeddings\n",
    "Word Embeddings is a NLP technique in which we try to capture the context, semantic meaning\n",
    "and inter relation of words with each other. It is done by creation of a word vector. Word vectors\n",
    "when projected upon a vector space can also show similarity between words.The technique or word\n",
    "embeddings which we discuss here today is Word-to-vec. We would be doing so with the help of\n",
    "Gensim which is another cool library like NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b3f5862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "[\"I\", \"love\", \"machine\", \"learning\"],\n",
    "[\"Word\", \"embeddings\", \"are\", \"important\"],\n",
    "[\"NLTK\", \"is\", \"a\", \"natural\", \"language\", \"toolkit\"]\n",
    "]\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(corpus, vector_size=100, window=5, min_count=1, sg=0)\n",
    "# Save the model for later use\n",
    "model.save(\"word2vec.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8f3e8002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'machine': [ 9.7702928e-03  8.1651136e-03  1.2809718e-03  5.0975787e-03\n",
      "  1.4081288e-03 -6.4551616e-03 -1.4280510e-03  6.4491653e-03\n",
      " -4.6173059e-03 -3.9930656e-03  4.9244044e-03  2.7130984e-03\n",
      " -1.8479753e-03 -2.8769434e-03  6.0107317e-03 -5.7167388e-03\n",
      " -3.2367026e-03 -6.4878250e-03 -4.2346325e-03 -8.5809948e-03\n",
      " -4.4697891e-03 -8.5112294e-03  1.4037776e-03 -8.6181965e-03\n",
      " -9.9166557e-03 -8.2016252e-03 -6.7726658e-03  6.6805850e-03\n",
      "  3.7845564e-03  3.5616636e-04 -2.9579818e-03 -7.4283206e-03\n",
      "  5.3341867e-04  4.9989222e-04  1.9561886e-04  8.5259555e-04\n",
      "  7.8633073e-04 -6.8160298e-05 -8.0070542e-03 -5.8702733e-03\n",
      " -8.3829118e-03 -1.3120425e-03  1.8206370e-03  7.4171280e-03\n",
      " -1.9634271e-03 -2.3252917e-03  9.4871549e-03  7.9704521e-05\n",
      " -2.4045217e-03  8.6048469e-03  2.6870037e-03 -5.3439722e-03\n",
      "  6.5881060e-03  4.5101536e-03 -7.0544672e-03 -3.2317400e-04\n",
      "  8.3448651e-04  5.7473574e-03 -1.7176545e-03 -2.8065301e-03\n",
      "  1.7484308e-03  8.4717153e-04  1.1928272e-03 -2.6342822e-03\n",
      " -5.9857843e-03  7.3229838e-03  7.5873756e-03  8.2963575e-03\n",
      " -8.5988473e-03  2.6364254e-03 -3.5599626e-03  9.6204039e-03\n",
      "  2.9037679e-03  4.6411133e-03  2.3856151e-03  6.6084778e-03\n",
      " -5.7432903e-03  7.8944126e-03 -2.4109220e-03 -4.5618857e-03\n",
      " -2.0609903e-03  9.7335577e-03 -6.8565905e-03 -2.1917201e-03\n",
      "  7.0009995e-03 -5.5749417e-05 -6.2949671e-03 -6.3935257e-03\n",
      "  8.9403950e-03  6.4295758e-03  4.7735930e-03 -3.2620477e-03\n",
      " -9.2676198e-03  3.7868882e-03  7.1605504e-03 -5.6328895e-03\n",
      " -7.8650126e-03 -2.9727400e-03 -4.9318983e-03 -2.3151112e-03]\n",
      "Most similar words to 'machine': [('natural', 0.17272792756557465), ('NLTK', 0.16694682836532593), ('are', 0.11117953062057495)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Load the Word2Vec model\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "# Explore the embeddings\n",
    "vector = model.wv[\"machine\"]\n",
    "similar_words = model.wv.most_similar(\"machine\", topn=3)\n",
    "print(\"Vector for 'machine':\", vector)\n",
    "print(\"Most similar words to 'machine':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9322314",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
